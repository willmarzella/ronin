<!-- ROLE: data_engineer -->

# William Marzella

Mont Albert North, VIC | 0413414869 | [williampmarzella@gmail.com](mailto:williampmarzella@gmail.com)

## Career Summary

Infrastructure-focused Data Engineer with six (6) years years of hands-on experience with Microsoft Azure cloud platform specializing in Azure VMs, AKS, Container Instances, Azure Storage, Azure Functions, Azure Synapse Analytics, Azure Data Factory, Azure SQL Database, and Cosmos DB. Extensive expertise in developing and optimizing data warehousing solutions using Snowflake on Azure with deep knowledge of performance tuning, query optimization, and cost reduction. Expert in implementing fault-tolerant ETL/ELT workflows leveraging industry-standard tools (dbt, Apache Airflow, Prefect, Azure Logic Apps) deployed on containerized Azure infrastructure. Proven track record implementing least-privilege Azure Active Directory policies, GitOps methodologies, and cloud governance frameworks ensuring regulatory compliance and operational efficiency. Strong background in infrastructure-as-code using Terraform with multi-environment deployments and CI/CD pipeline integration. Successfully executed large-scale Azure cloud migrations for enterprise clients, reducing operational costs by 30-50% while improving system performance and reliability.

## Highlight of Capabilities

- Eight (8) years years expert experience with Azure data infrastructure (Azure Data Lake Storage, Azure VMs, AKS, Container Instances, Azure Functions, Azure Synapse Analytics, Azure Data Factory, Azure SQL Database, Cosmos DB) and database technologies (PostgreSQL, SQL Server, Snowflake). Implemented enterprise-grade data lake architectures using Snowflake on Azure Storage, processing 50GB+ monthly data with optimized storage tiers and lifecycle policies while ensuring optimal query performance through clustering keys, materialized views, and schema evolution management.
- Seven (7) years years implementing enterprise-grade data processing solutions utilizing Apache Spark/PySpark with performance optimization for partition strategies, shuffle configurations, and memory management. Developed production-grade ETL/ELT workflows with dbt, Kafka, Snowpipe, and Stream processing for both batch and real-time applications, delivering robust solutions with comprehensive data quality frameworks and fault-tolerant architectures.
- Six (6) years years designing scalable data architectures using infrastructure-as-code (Terraform, ARM Templates, Azure Bicep) with modular design patterns, remote state management, and multi-environment deployments. Implemented DevOps best practices including GitOps, CI/CD pipelines (GitHub Actions, Azure DevOps), and comprehensive monitoring (Prometheus, Grafana, Application Insights, Azure Monitor) for observability across cloud environments, enabling cost-optimized, reliable data platforms capable of handling terabyte-scale workloads.

## Education

**University of Southern California** | Los Angeles, CA

*BS, Mechanical Engineering* | 2016 -- 2020

- Developed data acquisition system with Python and Docker deployed on Azure VMs processing 1000+ sensor readings/minute
- Implemented PostgreSQL databases on Azure SQL Database for equipment tracking with automated data pipelines using Python ETL scripts
- Created automated analysis workflows using Azure Functions with pandas and SQL queries reducing processing time by 65%

## Certifications

- Azure Data Engineer Associate -- *Microsoft* | 2025
- Terraform Certified Associate -- *HashiCorp* | 2025
- SnowProÂ® Core Certification -- *Snowflake* | 2025
- dbt Fundamentals Certification -- *dbt Labs* | 2024
- Azure Solutions Architect Expert -- *Microsoft* | 2023

## Experience (Full Time)

### **REST Industry Super** | Melbourne, VIC

Australia's leading industry superannuation fund managing over $70 billion in retirement assets for 1.7 million members across diverse sectors with award-winning digital services.

*Data Engineer* | March 2025 -- Present

Working on a cross-functional team of data engineers, data scientists, and business analysts to migrate legacy data warehouse to Snowflake while implementing a modern data platform with containerized microservices architecture. Developed data pipelines and infrastructure following GitOps methodologies and agile practices to support analytics and machine learning workloads.

- Designed and implemented migration strategy from legacy data warehouse to Snowflake on Azure, leveraging Azure Data Factory for extraction, Logic Apps for orchestration, and Azure Functions for transformation with dbt models
- Developed comprehensive data quality framework using dbt tests, Snowflake constraints, and Great Expectations, resulting in 99.8% data accuracy for critical financial reporting
- Built fault-tolerant ETL processes with Apache Airflow on AKS and Azure Data Factory workflows, deployed through Terraform, reducing pipeline failures by 75% and ensuring data availability for critical reporting deadlines
- Implemented Snowflake performance optimization techniques including star schema modeling, materialized views, clustering keys, and incremental models, decreasing query execution time by 65% and reducing compute costs by 40%

### **Alfab Pty Ltd** | Melbourne, VIC

Australia's premier manufacturing enterprise specializing in precision glass and aluminum fabrication with $10M annual revenue, operating state-of-the-art production facilities serving automotive and marine industries across Asia-Pacific.

*Senior Data Engineer* | October 2023 -- March 2025

Led the data engineering team modernizing manufacturing data infrastructure through implementation of cloud-native architectures. Collaborated closely with manufacturing operations and IT teams to enhance data platform capabilities while maintaining data governance requirements. Practiced agile methodologies with an offshore team of 2 developers to implement data pipelines and tools.

- Architected and deployed Azure and Snowflake data warehouse infrastructure processing 800GB of manufacturing data, utilizing Terraform for infrastructure management with remote state configuration, resulting in 35% Azure cost reduction ($150K annually)
- Designed and executed migration from legacy on-premise Oracle Data Warehouse to Snowflake on Azure, implementing Kimball dimensional models with dbt transformations, reducing monthly maintenance costs by 60% while improving query performance by 75%
- Developed containerized microservices architecture using Docker and Azure Kubernetes Service (AKS) for data processing applications, configuring proper resource allocation and auto-scaling policies, ensuring 99.9% system availability for critical manufacturing processes
- Engineered real-time data streaming solution with Apache Kafka on Azure Event Hubs implementing exactly-once processing for Change Data Capture, enabling predictive maintenance algorithms that reduced production line downtime by 22%
- Built and deployed CI/CD pipelines using Azure DevOps and GitHub Actions with orchestrated workflows in Apache Airflow on AKS, incorporating DAG optimization techniques that reduced deployment cycles from days to hours
- Created comprehensive monitoring solution with Prometheus, Grafana, and Azure Monitor tracking 15 key pipeline metrics with custom alerting, decreasing mean time to resolution from 4 hours to 45 minutes
- Developed Azure architecture documentation following infrastructure-as-code best practices in Azure DevOps Wiki, reducing onboarding time for new team members by 50%
- Implemented feature engineering pipeline using PySpark with optimized partition strategies and dbt transformations in Snowflake, enabling quality control models that reduced manufacturing defect rates by 18%
- Established DevOps practices with infrastructure-as-code (Terraform) for both Azure resources and Snowflake objects, enabling consistent cross-environment deployments and eliminating configuration drift

### **Tray.io** | San Diego, CA

Leading enterprise iPaaS (integration Platform as a Service) powering mission-critical data workflows for 200+ Fortune 500 companies processing over 1B monthly events, with specialized solutions for healthcare and fintech sectors requiring high-security, compliance-focused data integration.

*Platform Engineer* | April 2021 -- October 2023

Member of the platform engineering team developing customer analytics capabilities for healthcare and fintech clients using cloud-native architectures. Focused on processing customer interaction data with agile methodologies while adhering to compliance requirements. Contributed to solutions enabling customer behavior analysis across the data platform serving 200+ enterprise clients.

- Designed and implemented data pipelines using PySpark on Azure Databricks with Delta Lake tables, processing 10M+ daily customer events through optimized partition strategies, shuffle configuration, and memory management, achieving 99.8% data completeness
- Developed Azure-based customer journey analytics platform using Azure Databricks SQL and PySpark, which improved marketing retention metrics and preserved $2M+ in annual recurring revenue
- Architected Lambda architecture ETL workflows leveraging Apache Spark Structured Streaming for real-time data processing with exactly-once processing guarantees, reducing data-related support tickets by 35%
- Implemented and configured Kubernetes (AKS) environment for containerized data applications with Docker, establishing consistent deployment and scaling processes across development, staging, and production environments
- Created comprehensive data quality framework utilizing Azure Data Catalog and Great Expectations, ensuring 95% accuracy across 8 critical data sources for regulatory compliance
- Built observability infrastructure with Prometheus, Grafana, Application Insights, and Azure Monitor, reducing mean time to resolution from 30 minutes to 15 minutes and minimizing system downtime
- Developed machine learning feature pipelines using Azure Data Factory and Azure Machine Learning for customer churn prediction models, resulting in targeted interventions that improved customer retention by 18%
- Designed and deployed recommendation engine infrastructure using Apache Spark ML with A/B testing framework, increasing user engagement metrics by 28% and driving additional platform adoption
- Implemented infrastructure-as-code principles using Azure Bicep with proper CI/CD integration, reducing deployment time by 65% and virtually eliminating configuration errors

### **Chilton's Artisan Foods** | Melbourne, VIC

Award-winning specialty food manufacturer producing premium artisanal bakery products with $10M annual revenue, supplying Australia's top restaurants and gourmet retailers while maintaining farm-to-table supply chain with 100% locally-sourced Australian ingredients and sustainable manufacturing practices.

*Data Engineer* | July 2019 -- April 2021

First data hire supporting 12-person manufacturing operation with $10M annual revenue. Worked directly with production managers and finance team to understand requirements and implement initial data solutions following agile development practices.

- Designed and executed migration of legacy data warehouse to Snowflake on Azure with automated ETL processes using Python-based Azure Data Factory pipelines and Azure Blob Storage intelligent tiering, saving 25 hours weekly while enabling real-time visibility into $2M monthly inventory
- Developed ETL/ELT processes utilizing dbt for Snowflake transformations with comprehensive data tests, reducing manual data entry by 80% and establishing automated inventory tracking
- Implemented robust data validation logic in Snowflake using constraints and stored procedures, preventing $50K in monthly inventory waste through early detection of data anomalies
- Established Git-based version control with Azure DevOps Repos and comprehensive CI/CD pipelines, reducing deployment issues by 60% and ensuring consistent database change management
- Configured Docker containerization for development and production environments with optimized resource allocation, ensuring consistency across deployment stages
- Created business intelligence dashboards with Snowflake-connected Power BI, reducing monthly reporting time from 3 days to 4 hours while implementing version-controlled dbt models as single source of truth
- Built monitoring solution using Azure Monitor and Snowflake query monitoring to track 8 critical production KPIs, enabling 15% improvement in manufacturing efficiency through data-driven process optimization
- Implemented Terraform infrastructure-as-code with remote state management for Azure resource provisioning and Snowflake object creation, reducing configuration errors by 60% while enabling consistent cross-environment deployments

## Experience (Freelance)

### **Motis Group** | Melbourne, VIC

Independent technology consultancy offering specialized cloud and automation solutions (Part-time)

*Founder & Principal Infrastructure Engineer* | June 2022 -- Present

Provide strategic data infrastructure consulting to e-commerce and retail clients with $5M-50M annual revenue, focusing on cloud cost optimization and scalable architecture design.

- Designed and implemented modern data architecture using Snowflake on Azure for e-commerce clients, processing 50GB+ monthly data with optimized Azure Storage lifecycle policies and lifecycle policies, reducing storage costs by 45% while improving query performance
- Developed containerized ETL pipelines leveraging Docker, dbt, Kafka, Snowpipe, and Python UDFs, automating 12 daily inventory processes with 99.9% reliability and eliminating manual data processing
- Architected Azure Active Directory roles with least-privilege permissions and Snowflake RBAC security model, ensuring GDPR compliance for sensitive customer data and passing all security audits
- Implemented comprehensive FinOps practices for cloud resource optimization including Snowflake resource monitors, Azure Storage tiering, and VM right-sizing through Terraform, identifying $2K-5K monthly savings per client
- Created modular infrastructure-as-code using Terraform with multi-environment deployment strategies for both Azure resources and Snowflake objects, reducing environment setup time from 2 days to 2 hours
- Executed migration for retail client from on-premise SQL Server to Snowflake on Azure with dbt transformations and star schema modeling, reducing infrastructure costs by $2,000 monthly while improving report generation time from 30 minutes to 5 minutes
- Designed and implemented ML model deployment architecture using Azure Kubernetes Service, Snowflake Python UDFs, and Azure Machine Learning for retail demand forecasting, improving inventory planning accuracy by 25%
- Built automated data orchestration solutions using Apache Airflow on Azure Kubernetes Service with optimized DAGs and dbt Core for transformation workflows, enabling fully automated data pipelines with comprehensive monitoring and alerting
- Developed observability framework leveraging Prometheus, Grafana, and Azure Log Analytics for log aggregation and monitoring across client environments, reducing incident response time by 60%

